{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = '<API_KEY>'\n",
    "API_SECRET = '<API_SECRET>'\n",
    "CLUSTER_ID = '<CLUSTER_ID>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'io.confluent.kafka.server/received_bytes', 'description': 'The delta count of bytes received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': 'By', 'lifecycle_stage': 'GENERAL_AVAILABILITY', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'Name of the Kafka topic', 'key': 'topic'}, {'description': 'Partition number of the Kafka topic', 'key': 'partition'}]}, {'name': 'io.confluent.kafka.server/sent_bytes', 'description': 'The delta count of bytes sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': 'By', 'lifecycle_stage': 'GENERAL_AVAILABILITY', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'Name of the Kafka topic', 'key': 'topic'}, {'description': 'Partition number of the Kafka topic', 'key': 'partition'}]}, {'name': 'io.confluent.kafka.server/received_records', 'description': 'The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': '1', 'lifecycle_stage': 'GENERAL_AVAILABILITY', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'Name of the Kafka topic', 'key': 'topic'}, {'description': 'Partition number of the Kafka topic', 'key': 'partition'}]}, {'name': 'io.confluent.kafka.server/sent_records', 'description': 'The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': '1', 'lifecycle_stage': 'GENERAL_AVAILABILITY', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'Name of the Kafka topic', 'key': 'topic'}, {'description': 'Partition number of the Kafka topic', 'key': 'partition'}]}, {'name': 'io.confluent.kafka.server/retained_bytes', 'description': 'The current count of bytes retained by the cluster. The count is sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': 'By', 'lifecycle_stage': 'GENERAL_AVAILABILITY', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'Name of the Kafka topic', 'key': 'topic'}, {'description': 'Partition number of the Kafka topic', 'key': 'partition'}]}, {'name': 'io.confluent.kafka.server/active_connection_count', 'description': 'The count of active authenticated connections.', 'type': 'GAUGE_INT64', 'unit': '1', 'lifecycle_stage': 'PREVIEW', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}]}, {'name': 'io.confluent.kafka.server/request_count', 'description': 'The delta count of requests received over the network. Each sample is the number of requests received since the previousdata point. The count sampled every 60 seconds.', 'type': 'COUNTER_INT64', 'unit': '1', 'lifecycle_stage': 'PREVIEW', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}, {'description': 'The Kafka protocol request type (defined in https://kafka.apache.org/protocol#protocol_api_keys)', 'key': 'type'}]}, {'name': 'io.confluent.kafka.server/partition_count', 'description': 'The number of partitions.', 'type': 'GAUGE_INT64', 'unit': '1', 'lifecycle_stage': 'PREVIEW', 'labels': [{'description': 'ID of the Kafka cluster', 'key': 'cluster_id'}]}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.telemetry.confluent.cloud/v1/metrics/cloud/descriptors', auth=(API_KEY, API_SECRET))\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json()['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the available topics for a given metric in a specified interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'data': [], 'meta': {'pagination': {'page_size': 25}}}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"filter\": {\n",
    "        \"field\": \"metric.label.cluster_id\",\n",
    "        \"op\": \"EQ\",\n",
    "        \"value\": CLUSTER_ID\n",
    "    },\n",
    "    \"group_by\": [\n",
    "        \"metric.label.topic\"\n",
    "    ],\n",
    "    \"intervals\": [\n",
    "        \"2020-01-13T10:30:00-05:00/2020-01-13T11:00:00-05:00\"\n",
    "    ],\n",
    "    \"limit\": 25,\n",
    "    \"metric\": \"io.confluent.kafka.server/sent_bytes/delta\"\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/attributes',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "print(response)\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for bytes sent to consumers per minute grouped by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': []}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"aggregations\": [\n",
    "        {\n",
    "            \"agg\": \"SUM\",\n",
    "            \"metric\": \"io.confluent.kafka.server/sent_bytes\"\n",
    "        }\n",
    "    ],\n",
    "    \"filter\": {\n",
    "        \"filters\": [\n",
    "            {\n",
    "                \"field\": \"metric.label.cluster_id\",\n",
    "                \"op\": \"EQ\",\n",
    "                \"value\": CLUSTER_ID\n",
    "            }\n",
    "        ],\n",
    "        \"op\": \"AND\"\n",
    "    },\n",
    "    \"granularity\": \"PT1M\",\n",
    "    \"group_by\": [\n",
    "        \"metric.label.topic\"\n",
    "    ],\n",
    "    \"intervals\": [\n",
    "        \"2019-12-19T11:00:00-05:00/2019-12-19T11:05:00-05:00\"\n",
    "    ],\n",
    "    \"limit\": 25\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/query',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for bytes sent by producers per minute grouped by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': []}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"aggregations\": [\n",
    "        {\n",
    "            \"agg\": \"SUM\",\n",
    "            \"metric\": \"io.confluent.kafka.server/received_bytes\"\n",
    "        }\n",
    "    ],\n",
    "    \"filter\": {\n",
    "        \"filters\": [\n",
    "            {\n",
    "                \"field\": \"metric.label.cluster_id\",\n",
    "                \"op\": \"EQ\",\n",
    "                \"value\": CLUSTER_ID\n",
    "            }\n",
    "        ],\n",
    "        \"op\": \"AND\"\n",
    "    },\n",
    "    \"granularity\": \"PT1M\",\n",
    "    \"group_by\": [\n",
    "        \"metric.label.topic\"\n",
    "    ],\n",
    "    \"intervals\": [\n",
    "        \"2019-12-19T11:00:00-05:00/2019-12-19T11:05:00-05:00\"\n",
    "    ],\n",
    "    \"limit\": 25\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/query',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for max retained bytes per hour over 2 hours for topic named test-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': []}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"aggregations\": [\n",
    "        {\n",
    "            \"agg\": \"SUM\",\n",
    "            \"metric\": \"io.confluent.kafka.server/retained_bytes\"\n",
    "        }\n",
    "    ],\n",
    "    \"filter\": {\n",
    "        \"filters\": [\n",
    "            {\n",
    "                 \"field\": \"metric.label.topic\",\n",
    "                 \"op\": \"EQ\",\n",
    "                 \"value\": \"test-topic\"\n",
    "            },\n",
    "            {\n",
    "                \"field\": \"metric.label.cluster_id\",\n",
    "                \"op\": \"EQ\",\n",
    "                \"value\": CLUSTER_ID\n",
    "            }\n",
    "        ],\n",
    "        \"op\": \"AND\"\n",
    "    },\n",
    "    \"granularity\": \"PT1M\",\n",
    "    \"group_by\": [\n",
    "        \"metric.label.topic\"\n",
    "    ],\n",
    "    \"intervals\": [\n",
    "        \"2019-12-19T11:00:00-05:00/P0Y0M0DT2H0M0S\"\n",
    "    ],\n",
    "    \"limit\": 25\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/query',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for max retained bytes per hour over 2 hours for a cluster lkc-XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': []}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"aggregations\": [\n",
    "        {\n",
    "            \"agg\": \"SUM\",\n",
    "            \"metric\": \"io.confluent.kafka.server/retained_bytes\"\n",
    "        }\n",
    "    ],\n",
    "    \"filter\": {\n",
    "        \"filters\": [\n",
    "            {\n",
    "                \"field\": \"metric.label.cluster_id\",\n",
    "                \"op\": \"EQ\",\n",
    "                \"value\": CLUSTER_ID\n",
    "            }\n",
    "        ],\n",
    "        \"op\": \"AND\"\n",
    "    },\n",
    "    \"granularity\": \"PT1H\",\n",
    "    \"group_by\": [\n",
    "        \"metric.label.cluster_id\"\n",
    "    ],\n",
    "    \"intervals\": [\n",
    "        \"2019-12-19T11:00:00-05:00/P0Y0M0DT2H0M0S\"\n",
    "    ],\n",
    "    \"limit\": 5\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/query',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for metrics currently available for a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'metric': 'io.confluent.kafka.server/retained_bytes'}, {'metric': 'io.confluent.kafka.server/active_connection_count'}, {'metric': 'io.confluent.kafka.server/partition_count'}, {'metric': 'io.confluent.kafka.server/request_count'}], 'meta': {'pagination': {'page_size': 100}}}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"filter\": {\n",
    "    \"field\": \"metric.label.cluster_id\",\n",
    "    \"op\": \"EQ\",\n",
    "    \"value\": CLUSTER_ID\n",
    "  }\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.telemetry.confluent.cloud/v1/metrics/cloud/available',\n",
    "                        auth=(API_KEY, API_SECRET),\n",
    "                       json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('oops, something went wrong!!')\n",
    "else:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
